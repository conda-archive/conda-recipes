{% set name = 'sparklyr' %}
{% set version = '0.5.1' %}

{% set posix = 'm2-' if win else '' %}
{% set native = 'm2w64-' if win else '' %}

package:
  name: r-{{ name|lower }}
  version: {{ version|replace("-", "_") }}

source:
  fn: {{ name }}_{{ version }}.tar.gz
  url:
    - https://cran.r-project.org/src/contrib/{{ name }}_{{ version }}.tar.gz
    - https://cran.r-project.org/src/contrib/Archive/{{ name }}/{{ name }}_{{ version }}.tar.gz


  # You can add a hash for the file here, like md5 or sha1
  # md5: 49448ba4863157652311cc5ea4fea3ea
  # sha1: 3bcfbee008276084cbb37a2b453963c61176a322
  # patches:
   # List any patch files here
   # - fix.patch

build:
  # If this is a new build for the same version, increment the build number.
  number: 0

  # This is required to make R link correctly on Linux.
  rpaths:
    - lib/R/lib/
    - lib/

# Suggests: testthat, RCurl, janeaustenr
requirements:
  build:
    - r-base
    - r-dbi >=0.4.1
    - r-assertthat
    - r-base64enc
    - r-config
    - r-digest
    - r-dplyr >=0.5.0
    - r-httr
    - r-jsonlite
    - r-lazyeval >=0.2.0
    - r-rappdirs
    - r-readr >=0.2.0
    - r-rprojroot
    - r-withr

  run:
    - r-base
    - r-dbi >=0.4.1
    - r-assertthat
    - r-base64enc
    - r-config
    - r-digest
    - r-dplyr >=0.5.0
    - r-httr
    - r-jsonlite
    - r-lazyeval >=0.2.0
    - r-rappdirs
    - r-readr >=0.2.0
    - r-rprojroot
    - r-withr

test:
  commands:
    # You can put additional test commands to be run here.
    - $R -e "library('sparklyr')"  # [not win]
    - "\"%R%\" -e \"library('sparklyr')\""  # [win]

  # You can also put a file called run_test.py, run_test.sh, or run_test.bat
  # in the recipe that will be run at test time.

  # requires:
    # Put any additional test requirements here.

about:
  home: http://spark.rstudio.com
  license: Apache License 2.0 | file LICENSE
  summary: R interface to Apache Spark, a fast and general engine for big data processing, see
    <http://spark.apache.org>. This package supports connecting to local and remote
    Apache Spark clusters, provides a 'dplyr' compatible back-end, and provides an interface
    to Spark's built-in machine learning algorithms.
  license_family: APACHE

# The original CRAN metadata for this package was:

# Package: sparklyr
# Type: Package
# Title: R Interface to Apache Spark
# Version: 0.5.1
# Authors@R: c( person("Javier", "Luraschi", email = "javier@rstudio.com", role = c("aut", "cre")), person("Kevin", "Ushey", role = "aut", email = "kevin@rstudio.com"), person("JJ", "Allaire", role = "aut", email = "jj@rstudio.com"), person(family = "RStudio", role = c("cph")), person(family = "The Apache Software Foundation", role = c("aut", "cph")) )
# Maintainer: Javier Luraschi <javier@rstudio.com>
# Description: R interface to Apache Spark, a fast and general engine for big data processing, see <http://spark.apache.org>. This package supports connecting to local and remote Apache Spark clusters, provides a 'dplyr' compatible back-end, and provides an interface to Spark's built-in machine learning algorithms.
# License: Apache License 2.0 | file LICENSE
# URL: http://spark.rstudio.com
# BugReports: https://github.com/rstudio/sparklyr/issues
# LazyData: TRUE
# RoxygenNote: 5.0.1
# Depends: R (>= 3.1.2)
# Imports: methods, lazyeval (>= 0.2.0), dplyr (>= 0.5.0), DBI (>= 0.4.1), readr (>= 0.2.0), digest, config, rappdirs, assertthat, rprojroot, withr, httr, jsonlite, base64enc
# Suggests: testthat, RCurl, janeaustenr
# NeedsCompilation: no
# Packaged: 2016-12-19 10:06:58 UTC; javierluraschi
# Author: Javier Luraschi [aut, cre], Kevin Ushey [aut], JJ Allaire [aut], RStudio [cph], The Apache Software Foundation [aut, cph]
# Repository: CRAN
# Date/Publication: 2016-12-19 15:56:06

# See
# http://docs.continuum.io/conda/build.html for
# more information about meta.yaml
