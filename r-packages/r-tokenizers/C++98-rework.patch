From d2518d3cbeed637bbdc9af71d9bf001713e0a2ed Mon Sep 17 00:00:00 2001
From: Oliver Keyes <ironholds@gmail.com>
Date: Thu, 30 Mar 2017 21:14:25 -0700
Subject: [PATCH] C++98 rework

---
 DESCRIPTION               |  4 +++-
 R/RcppExports.R           |  2 +-
 man/basic-tokenizers.Rd   |  5 ++---
 man/ngram-tokenizers.Rd   |  1 -
 man/shingle-tokenizers.Rd |  1 -
 man/stem-tokenizers.Rd    |  1 -
 man/stopwords.Rd          |  1 -
 man/tokenizers.Rd         |  1 -
 src/Makevars              |  2 --
 src/Makevars.win          |  2 --
 src/RcppExports.cpp       | 26 +++++++++++++-------------
 src/shingle_ngrams.cpp    | 36 ++++++++++++++++++++----------------
 12 files changed, 39 insertions(+), 43 deletions(-)
 delete mode 100644 src/Makevars
 delete mode 100644 src/Makevars.win

diff --git a/R/RcppExports.R b/R/RcppExports.R
index f3bb1d4..a5ebcc5 100644
--- a/R/RcppExports.R
+++ b/R/RcppExports.R
@@ -1,4 +1,4 @@
-# This file was generated by Rcpp::compileAttributes
+# Generated by using Rcpp::compileAttributes() -> do not edit by hand
 # Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393
 
 generate_ngrams_batch <- function(documents_list, ngram_min, ngram_max, stopwords = character(), ngram_delim = " ") {
diff --git a/man/basic-tokenizers.Rd b/man/basic-tokenizers.Rd
index 2ffdaa0..c47f73a 100644
--- a/man/basic-tokenizers.Rd
+++ b/man/basic-tokenizers.Rd
@@ -3,11 +3,11 @@
 \name{basic-tokenizers}
 \alias{basic-tokenizers}
 \alias{tokenize_characters}
+\alias{tokenize_words}
+\alias{tokenize_sentences}
 \alias{tokenize_lines}
 \alias{tokenize_paragraphs}
 \alias{tokenize_regex}
-\alias{tokenize_sentences}
-\alias{tokenize_words}
 \title{Basic tokenizers}
 \usage{
 tokenize_characters(x, lowercase = TRUE, strip_non_alphanum = TRUE,
@@ -81,4 +81,3 @@ tokenize_lines(song)
 tokenize_characters(song)
 tokenize_regex("A,B,C,D,E", pattern = ",")
 }
-
diff --git a/man/ngram-tokenizers.Rd b/man/ngram-tokenizers.Rd
index 5db5ce6..e3bcb47 100644
--- a/man/ngram-tokenizers.Rd
+++ b/man/ngram-tokenizers.Rd
@@ -78,4 +78,3 @@ tokenize_ngrams(song, n = 4)
 tokenize_ngrams(song, n = 4, n_min = 1)
 tokenize_skip_ngrams(song, n = 4, k = 2)
 }
-
diff --git a/man/shingle-tokenizers.Rd b/man/shingle-tokenizers.Rd
index e83d113..e1cafc9 100644
--- a/man/shingle-tokenizers.Rd
+++ b/man/shingle-tokenizers.Rd
@@ -49,4 +49,3 @@ tokenize_character_shingles(x, n = 5, strip_non_alphanum = FALSE)
 tokenize_character_shingles(x, n = 5, n_min = 3, strip_non_alphanum = FALSE)
 
 }
-
diff --git a/man/stem-tokenizers.Rd b/man/stem-tokenizers.Rd
index b498209..30b5d13 100644
--- a/man/stem-tokenizers.Rd
+++ b/man/stem-tokenizers.Rd
@@ -59,4 +59,3 @@ tokenize_word_stems(song)
 \seealso{
 \code{\link[SnowballC]{wordStem}}
 }
-
diff --git a/man/stopwords.Rd b/man/stopwords.Rd
index f01ad07..8e2627f 100644
--- a/man/stopwords.Rd
+++ b/man/stopwords.Rd
@@ -19,4 +19,3 @@ stopwords("de")
 \references{
 The stopword lists are a subset of the stopword lists available in the \href{https://github.com/apache/lucene-solr/tree/master/solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/lang}{Apache Lucene/Solr} project, available under the Apache 2.0 license.
 }
-
diff --git a/man/tokenizers.Rd b/man/tokenizers.Rd
index 6fef6dd..dd2e9e1 100644
--- a/man/tokenizers.Rd
+++ b/man/tokenizers.Rd
@@ -18,4 +18,3 @@ vector, where each element in the list are the tokens generated by the
 function. If the input character vector or list is named, then the names are
 preserved.
 }
-
diff --git a/src/Makevars b/src/Makevars
deleted file mode 100644
index 64c87f7..0000000
--- a/src/Makevars
+++ /dev/null
@@ -1,2 +0,0 @@
-CXX_STD = CXX11
-
diff --git a/src/Makevars.win b/src/Makevars.win
deleted file mode 100644
index 64c87f7..0000000
--- a/src/Makevars.win
+++ /dev/null
@@ -1,2 +0,0 @@
-CXX_STD = CXX11
-
diff --git a/src/RcppExports.cpp b/src/RcppExports.cpp
index e525227..cc0daf3 100644
--- a/src/RcppExports.cpp
+++ b/src/RcppExports.cpp
@@ -1,4 +1,4 @@
-// This file was generated by Rcpp::compileAttributes
+// Generated by using Rcpp::compileAttributes() -> do not edit by hand
 // Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393
 
 #include <Rcpp.h>
@@ -6,30 +6,30 @@
 using namespace Rcpp;
 
 // generate_ngrams_batch
-ListOf<CharacterVector> generate_ngrams_batch(const ListOf<const CharacterVector> documents_list, const uint32_t ngram_min, const uint32_t ngram_max, const CharacterVector stopwords, const String ngram_delim);
+ListOf<CharacterVector> generate_ngrams_batch(const ListOf<const CharacterVector> documents_list, const int ngram_min, const int ngram_max, CharacterVector stopwords, const String ngram_delim);
 RcppExport SEXP tokenizers_generate_ngrams_batch(SEXP documents_listSEXP, SEXP ngram_minSEXP, SEXP ngram_maxSEXP, SEXP stopwordsSEXP, SEXP ngram_delimSEXP) {
 BEGIN_RCPP
-    Rcpp::RObject __result;
-    Rcpp::RNGScope __rngScope;
+    Rcpp::RObject rcpp_result_gen;
+    Rcpp::RNGScope rcpp_rngScope_gen;
     Rcpp::traits::input_parameter< const ListOf<const CharacterVector> >::type documents_list(documents_listSEXP);
-    Rcpp::traits::input_parameter< const uint32_t >::type ngram_min(ngram_minSEXP);
-    Rcpp::traits::input_parameter< const uint32_t >::type ngram_max(ngram_maxSEXP);
-    Rcpp::traits::input_parameter< const CharacterVector >::type stopwords(stopwordsSEXP);
+    Rcpp::traits::input_parameter< const int >::type ngram_min(ngram_minSEXP);
+    Rcpp::traits::input_parameter< const int >::type ngram_max(ngram_maxSEXP);
+    Rcpp::traits::input_parameter< CharacterVector >::type stopwords(stopwordsSEXP);
     Rcpp::traits::input_parameter< const String >::type ngram_delim(ngram_delimSEXP);
-    __result = Rcpp::wrap(generate_ngrams_batch(documents_list, ngram_min, ngram_max, stopwords, ngram_delim));
-    return __result;
+    rcpp_result_gen = Rcpp::wrap(generate_ngrams_batch(documents_list, ngram_min, ngram_max, stopwords, ngram_delim));
+    return rcpp_result_gen;
 END_RCPP
 }
 // skip_ngrams
 CharacterVector skip_ngrams(CharacterVector words, int n, int k);
 RcppExport SEXP tokenizers_skip_ngrams(SEXP wordsSEXP, SEXP nSEXP, SEXP kSEXP) {
 BEGIN_RCPP
-    Rcpp::RObject __result;
-    Rcpp::RNGScope __rngScope;
+    Rcpp::RObject rcpp_result_gen;
+    Rcpp::RNGScope rcpp_rngScope_gen;
     Rcpp::traits::input_parameter< CharacterVector >::type words(wordsSEXP);
     Rcpp::traits::input_parameter< int >::type n(nSEXP);
     Rcpp::traits::input_parameter< int >::type k(kSEXP);
-    __result = Rcpp::wrap(skip_ngrams(words, n, k));
-    return __result;
+    rcpp_result_gen = Rcpp::wrap(skip_ngrams(words, n, k));
+    return rcpp_result_gen;
 END_RCPP
 }
diff --git a/src/shingle_ngrams.cpp b/src/shingle_ngrams.cpp
index 755ee9c..f194652 100644
--- a/src/shingle_ngrams.cpp
+++ b/src/shingle_ngrams.cpp
@@ -2,9 +2,9 @@
 using namespace Rcpp;
 
 // calculates size of the ngram vector
-inline  size_t get_ngram_seq_len(uint32_t input_len, uint32_t ngram_min, uint32_t ngram_max) {
+inline size_t get_ngram_seq_len(int input_len, int ngram_min, int ngram_max) {
 
-  uint32_t out_ngram_len_adjust = 0;
+  int out_ngram_len_adjust = 0;
   for (size_t i = ngram_min - 1; i < ngram_max; i++)
     out_ngram_len_adjust += i;
   if(input_len < ngram_min)
@@ -14,25 +14,26 @@ inline  size_t get_ngram_seq_len(uint32_t input_len, uint32_t ngram_min, uint32_
 }
 
 CharacterVector generate_ngrams_internal(const CharacterVector terms_raw,
-                                const uint32_t ngram_min,
-                                const uint32_t ngram_max,
-                                RCPP_UNORDERED_SET<std::string> &stopwords,
+                                const int ngram_min,
+                                const int ngram_max,
+                                std::set<std::string> &stopwords,
                                 // pass buffer by reference to avoid memory allocation
                                 // on each iteration
-                                std::vector<std::string> &terms_filtered_buffer,
+                                std::deque<std::string> &terms_filtered_buffer,
                                 const std::string ngram_delim) {
   // clear buffer from previous iteration result
   terms_filtered_buffer.clear();
   std::string term;
   // filter out stopwords
-  for (auto it: terms_raw) {
-    term  = as<std::string>(it);
+  for (size_t i = 0; i < terms_raw.size(); i++) {
+    term  = as<std::string>(terms_raw[i]);
     if(stopwords.find(term) == stopwords.end())
       terms_filtered_buffer.push_back(term);
   }
 
-  uint32_t len = terms_filtered_buffer.size();
+  int len = terms_filtered_buffer.size();
   size_t ngram_out_len = get_ngram_seq_len(len, ngram_min, std::min(ngram_max, len));
+
   CharacterVector result(ngram_out_len);
 
   std::string k_gram;
@@ -65,20 +66,23 @@ CharacterVector generate_ngrams_internal(const CharacterVector terms_raw,
 
 // [[Rcpp::export]]
 ListOf<CharacterVector> generate_ngrams_batch(const ListOf<const CharacterVector> documents_list,
-                                              const uint32_t ngram_min,
-                                              const uint32_t ngram_max,
-                                              const CharacterVector stopwords = CharacterVector(),
+                                              const int ngram_min,
+                                              const int ngram_max,
+                                              CharacterVector stopwords = CharacterVector(),
                                               const String ngram_delim = " ") {
 
-  std::vector<std::string> terms_filtered_buffer;
+  std::deque<std::string> terms_filtered_buffer;
   const std::string std_string_delim = ngram_delim.get_cstring();
   size_t n_docs = documents_list.size();
   List result(n_docs);
   CharacterVector terms;
 
-  RCPP_UNORDERED_SET<std::string> stopwords_set;
-  for(auto it:stopwords)
-    stopwords_set.insert(as<std::string>(it));
+  std::set<std::string> stopwords_set;
+  for(size_t i = 0; i < stopwords.size(); i++){
+    if(stopwords[i] != NA_STRING){
+      stopwords_set.insert(as<std::string>(stopwords[i]));
+    }
+  }
 
   for (size_t i_document = 0; i_document < n_docs; i_document++) {
     terms = documents_list[i_document];
